{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting BERT Models (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to interpret Bert models using  `Captum` library. In this particular case study we focus on a fine-tuned Question Answering model on SQUAD dataset using transformers library from Hugging Face: https://huggingface.co/transformers/\n",
    "\n",
    "We show how to use interpretation hooks to examine and better understand embeddings, sub-embeddings, bert, and attention layers. \n",
    "\n",
    "Note: Before running this tutorial, please install `seaborn`, `pandas` and `matplotlib`, `transformers`(from hugging face) python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://captum.ai/tutorials/Bert_SQUAD_Interpret\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "from tqdm.notebook import tqdm\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "\n",
    "from models import BertForGuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to fine-tune BERT model on SQUAD dataset. This can be easiy accomplished by following the steps described in hugging face's official web site: https://github.com/huggingface/transformers#run_squadpy-fine-tuning-on-squad-for-question-answering \n",
    "\n",
    "Note that the fine-tuning is done on a `bert-base-uncased` pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we pretrain the model, we can load the tokenizer and pre-trained BERT model using the commands described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to perform forward pass of the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None, training_head=None):\n",
    "    return model(input_ids=inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, training_head = training_head, device=device)\n",
    "\n",
    "\n",
    "def guilt_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, training_head=None):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask, training_head=training_head)\n",
    "    return pred[0]\n",
    "# input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "#                 position_ids=None, head_mask=None, inputs_embeds=None, labels=None, training_head=[-1], with_token_cls=False, token_labels=None,  device='cpu',  highlight_ratio=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a custom forward function that will allow us to access the start and end postitions of our prediction using `position` input argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute attributions with respect to the `BertEmbeddings` layer.\n",
    "\n",
    "To do so, we need to define baselines / references, numericalize both the baselines and the inputs. We will define helper functions to achieve that.\n",
    "\n",
    "The cell below defines numericalized special tokens that will be later used for constructing inputs and corresponding baselines/references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a set of helper function for constructing references / baselines for word tokens, token types and position ids. We also provide separate helper functions that allow to construct the sub-embeddings and corresponding baselines / references for all sub-embeddings of `BertEmbeddings` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids):\n",
    "    # return token_type_ids, ref_token_type_ids\n",
    "    seq_len = input_ids.size(1)\n",
    "    return torch.zeros(seq_len, device=device, dtype=torch.long), torch.zeros(seq_len, device=device, dtype=torch.long)\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = (attributions - torch.mean(attributions))/ torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the `question - text` pair that we'd like to use as an input for our Bert model and interpret what the model was forcusing on when predicting an answer to the question from given input text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "training_head = ['1']\n",
    "model_path = '<path_to_model_dump>'\n",
    "\n",
    "# load model\n",
    "model = BertForGuilt.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('<path_to_test_dataset>') as infile:\n",
    "    for line in infile:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's numericalize the question, the input text and generate corresponding baselines / references for all three sub-embeddings (word, token type and position embeddings) types using our helper functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_score_vises = {}\n",
    "for row in tqdm(data[:10]):\n",
    "    text = row['story_clean']\n",
    "    ground_truth = np.mean([i.get('suspect_committedCrime', 0) for i in row['data'].values()])\n",
    "    if np.isnan(ground_truth):\n",
    "        continue\n",
    "    input_ids, ref_input_ids  = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
    "    token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids)\n",
    "    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "    attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "    score = predict(input_ids, attention_mask=attention_mask, training_head=training_head)\n",
    "\n",
    "    lig = LayerIntegratedGradients(guilt_forward_func, model.bert.embeddings)\n",
    "\n",
    "    attributions_score, delta_score = lig.attribute(inputs=input_ids,\n",
    "                                      baselines=ref_input_ids,\n",
    "                                      additional_forward_args=(None, None, None, training_head),\n",
    "                                      return_convergence_delta=True)\n",
    "    attributions_score_sum = summarize_attributions(attributions_score)\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    score_vis = viz.VisualizationDataRecord(\n",
    "                            attributions_score_sum,\n",
    "                            np.round(score[0].item(),3),\n",
    "                            -1,\n",
    "                            -1,\n",
    "                            str(ground_truth),\n",
    "                            attributions_score_sum.sum(),       \n",
    "                            all_tokens,\n",
    "                            delta_score)\n",
    "    \n",
    "    all_score_vises[row['story_id']] = score_vis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_parser(string, highlights, tokenizer, source):\n",
    "    # return list of token, indices, and highlight score\n",
    "    highlights = [i for i in highlights if i]\n",
    "    if len(highlights) == 0:\n",
    "        return None\n",
    "    highlights = [hl[source if source else 0] for hl in highlights]\n",
    "    assert len(string) == len(highlights[0])\n",
    "    highlights = [[int(i) for i in hl] for hl in highlights]\n",
    "    string_splited = string_split(string)\n",
    "    wordpiece_tokens = sum([wordpiece_with_indices(tokenizer, tok, start) for tok, start, end in string_splited], [])\n",
    "    highlights = [[np.mean(highlight[start:end]) for _, start, end in wordpiece_tokens] for highlight in highlights]\n",
    "    highlight_mean = list(np.mean(highlights, axis=0))\n",
    "    return highlight_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.visualize_text(all_score_vises.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-zijwang",
   "language": "python",
   "name": "py37-zijwang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
